{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f98c6b4-4a90-4f64-adae-4ce2b75e4d57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "IPython.notebook.set_autosave_interval(180000)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 180 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-e4476ecac24a>:12: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cat/.conda/envs/donato/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "%matplotlib tk\n",
    "%autosave 180\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import nest_asyncio\n",
    "%config Completer.use_jedi = False\n",
    "\n",
    "#\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "# \n",
    "import numpy as np\n",
    "import os\n",
    "import scipy\n",
    "\n",
    "# add root directory to be able to import packages\n",
    "# todo: make all packages installable so they can be called/imported by environment\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "sys.path.append(module_path)\n",
    "\n",
    "from utils.calcium import calcium\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "28da3655-1a7f-43c0-ba3b-f42276c20e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRocessing:  DON-011733\n",
      "processing:  20230309\n",
      "... NOTE: dedupliation method should be generally used with correlation data ... \n",
      "     standalone version here is for testing purposes only ... \n",
      "Removed cells:  33\n",
      "number of contours: 299\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "############################################\n",
    "################# Load data ################\n",
    "############################################\n",
    "\n",
    "# #\n",
    "# def get_sessions_from_directory_listing(root_dir,\n",
    "#                                        animal_id):\n",
    "    \n",
    "#     sessions = os.listdir(root_dir+animal_id)\n",
    "#     print (\"sessions: \", sessions)\n",
    "    \n",
    "#     return sessions\n",
    "    \n",
    "#\n",
    "\n",
    "animal_ids = [\n",
    "    'DON-011733',\n",
    "    #'DON-002865',\n",
    "    #'DON-003165'    \n",
    "    #'DON-003343'\n",
    "    #'DON-006084'\n",
    "    #'DON-006085'\n",
    "    #'DON-006087',\n",
    "    #'DON-009192',\n",
    "    #'DON-010473',\n",
    "    #'DON-010477'\n",
    "]\n",
    "              \n",
    "#\n",
    "root_dir = '/media/cat/4TB/donato/'\n",
    "root_dir = '/home/cat/data/bmi/'\n",
    "animal_id = animal_ids[0]\n",
    "dir_ = '/002P-F/tif/'\n",
    "\n",
    "#session = 'DON-002865_202102116'\n",
    "#session = 'DON-003343_20210220'\n",
    "session = '20230309'\n",
    "#session = '20210601'\n",
    "#sessions = get_sessions_from_directory_listing(root_dir,\n",
    "#                                              animal_id)\n",
    "\n",
    "print (\"PRocessing: \", animal_id)\n",
    "#\n",
    "#\n",
    "print (\"processing: \", session)\n",
    "c = calcium.Calcium()\n",
    "c.root_dir = root_dir\n",
    "c.data_dir = os.path.join(root_dir, \n",
    "                          animal_id, \n",
    "                          session\n",
    "                          #+dir_\n",
    "                          , \n",
    "                          #'suite2p',\n",
    "                          'plane0')\n",
    "c.animal_id = animal_id\n",
    "c.session = session\n",
    "c.detrend_model_order = 1\n",
    "\n",
    "#\n",
    "c.load_suite2p()\n",
    "c.save_python = True\n",
    "c.save_matlab = False\n",
    "\n",
    "# \n",
    "c.load_binarization()\n",
    "binarization_method = 'upphase'\n",
    "if binarization_method=='onphase':\n",
    "    traces = c.F_onphase_bin\n",
    "elif binarization_method=='upphase':\n",
    "    traces = c.F_upphase_bin\n",
    "else:\n",
    "    print (\"METHOD NOT FOUND\")\n",
    "\n",
    "# print (\"binarized data: \", traces.shape)\n",
    "\n",
    "###################################################################\n",
    "########## cleanup cells + compute pairwise correlations ##########\n",
    "###################################################################\n",
    "c.subselect_moving_only = False\n",
    "c.subselect_quiescent_only = False\n",
    "c.load_footprints()\n",
    "# c.corr_parallel_flag = True\n",
    "# c.zscore = True                     # here we compute correlations based on N shuffles of the data and zscore\n",
    "#                                     # otherwise we save the raw correlation also\n",
    "# c.n_tests_zscore = 1000\n",
    "\n",
    "# #\n",
    "\n",
    "# # \n",
    "# c.n_cores = 32\n",
    "# c.recompute_correlation = False\n",
    "# c.binning_window = 30        # binning window in frames\n",
    "# c.subsample = 1              # subsample traces by this factor\n",
    "# c.scale_by_DFF = True        # scale traces by DFF\n",
    "# c.compute_correlations()\n",
    "\n",
    "# remove duplicate neurons requires correlation array to be computed first; it uses physical overalp but also correlation threshold\n",
    "\n",
    "#\n",
    "print (\"... NOTE: dedupliation method should be generally used with correlation data ... \")\n",
    "print (\"     standalone version here is for testing purposes only ... \")\n",
    "c.zscore = False\n",
    "c.shuffle_data = False\n",
    "c.deduplication_use_correlations = False\n",
    "c.correlation_datatype = 'upphase'      # filtered vs. upphase\n",
    "\n",
    "# \n",
    "c.deduplication_method = 'overlap'      # 'overlap'; 'centre_distance'\n",
    "c.corr_min_distance = 8                 # min distance for centre_distance method - NOT USED HERE\n",
    "c.corr_max_percent_overlap = 0.25       # max overlap for overlap method\n",
    "c.corr_threshold = 0.3                  # max correlation allowed for high overlap\n",
    "c.zscore_threshold = 3.0                # zscore threshold for high overlap\n",
    "\n",
    "#  \n",
    "c.corr_delete_method = 'highest_connected_no_corr' #'highest_connected', lowest_snr', 'highest_connected_no_corr' <- this skips correlation check\n",
    "c.recompute_deduplication = True\n",
    "c.recompute_overlap = False    #this is not required to be redone, it's just once\n",
    "c.make_correlation_dirs()\n",
    "c.remove_duplicate_neurons()     # this removes duplicate neurons and saves non-duplicate version of correlation array\n",
    "\n",
    "#\n",
    "print (\"DONE\")\n",
    "\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6c852843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test1\n"
     ]
    }
   ],
   "source": [
    "D = {\n",
    "    'data_dir': 'test1',\n",
    "}\n",
    "\n",
    "# extract value for key \"data_dir\" from dictionary D\n",
    "data_dir = D['data_dir']\n",
    "print (data_dir)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02694f74-069d-4eaf-92b1-f90764494524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(806, 806)\n"
     ]
    }
   ],
   "source": [
    "#d = np.load('/media/cat/4TB/donato/DON-002865/DON-002865_20210210/002P-F/tif/suite2p/plane0/allcell_correlation_array_filtered.npy')\n",
    "#d = np.load('/media/cat/4TB/donato/DON-002865/DON-002865_20210210/002P-F/tif/suite2p/plane0/allcell_correlation_array_upphase.npy')\n",
    "d = np.load('/media/cat/4TB/donato/DON-002865/DON-002865_20210210/002P-F/tif/suite2p/plane0/goodcell_correlations_array_post_deduplication_filtered.npy')\n",
    "print (d.shape)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(d[:,:])\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a10b011-9b31-4736-89d8-a58d8de82440",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:14<00:00, 34.30it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_corr2(temp1, temp2, dynamic_corr_flag, n_tests=1000):\n",
    "    # \n",
    "    # check if all values are the same\n",
    "    if np.all(temp1==temp1[0]):\n",
    "        corr = [np.nan,1]\n",
    "        return corr, [np.nan], [np.nan]\n",
    "\n",
    "    if np.all(temp2==temp2[0]):\n",
    "        corr = [np.nan,1]\n",
    "        return corr, [np.nan], [np.nan]\n",
    "\n",
    "    # if using dynamic correlation we need to compute the correlation for 1000 shuffles\n",
    "    corr_array = []\n",
    "    corr = scipy.stats.pearsonr(temp1, temp2)\n",
    "    corr_array.append(corr[0])\n",
    "                                \n",
    "\n",
    "    if dynamic_corr_flag:\n",
    "        corr_s = []\n",
    "        for k in range(n_tests):\n",
    "            # choose a random index ranging from 0 to the length of the array minus 1\n",
    "            idx = np.random.randint(-temp2.shape[0], temp2.shape[0],1)\n",
    "            #idx = np.random.randint(temp2.shape[0])\n",
    "            temp2_shuffled = np.roll(temp2, idx)\n",
    "            corr_s = scipy.stats.pearsonr(temp1, temp2_shuffled)\n",
    "            corr_array.append(corr_s[0])\n",
    "\n",
    "        # compute z-score\n",
    "        corr_z = stats.zscore(corr_array)\n",
    "\n",
    "        #\n",
    "        corr = [corr[0], corr_z[0]]\n",
    "        \n",
    "    return corr, corr_z, corr_array\n",
    "\n",
    "####################################\n",
    "data  = np.load('/media/cat/4TB/donato/DON-006084/20210523/002P-F/tif/suite2p/plane0/binarized_traces.npz',\n",
    "                allow_pickle=True)\n",
    "from scipy import stats\n",
    "#\n",
    "subsample = 5\n",
    "dynamic_corr_flag = True\n",
    "scale_by_DFF = True\n",
    "rasters_DFF = data['F_upphase']\n",
    "DFF = data['DFF']\n",
    "binning_window = 30\n",
    "\n",
    "#\n",
    "k = 1\n",
    "temp1 = rasters[k][::subsample]\n",
    "\n",
    "# scale by rasters_DFF\n",
    "if scale_by_DFF:\n",
    "    temp1 = temp1*rasters_DFF[0][::subsample]\n",
    "\n",
    "# bin data in chunks of size binning_window\n",
    "if binning_window!=1:\n",
    "    tt = []\n",
    "    for q in range(0, temp1.shape[0], binning_window):\n",
    "        temp = np.sum(temp1[q:q + binning_window])\n",
    "        tt.append(temp)\n",
    "    temp1 = np.array(tt)\n",
    "\n",
    "#\n",
    "cz = []\n",
    "from tqdm import trange\n",
    "for p in trange(500):\n",
    "    temp2 = rasters[p][::subsample]\n",
    "\n",
    "    # scale by rasters_DFF\n",
    "    if scale_by_DFF:\n",
    "        temp2 = temp2*rasters_DFF[p][::subsample]\n",
    "\n",
    "    # \n",
    "    if binning_window!=1:\n",
    "        tt = []\n",
    "        for q in range(0, temp2.shape[0], binning_window):\n",
    "            temp = np.sum(temp2[q:q + binning_window])\n",
    "            tt.append(temp)\n",
    "        temp2 = np.array(tt)\n",
    "\n",
    "    #\n",
    "    corr, corr_z, corr_array = get_corr2(temp1, temp2, dynamic_corr_flag)\n",
    "\n",
    "    #print (\"corr: \", corr)\n",
    "    cz.append(corr_z[0])\n",
    "\n",
    "# plot cz\n",
    "plt.figure()\n",
    "plt.plot(cz)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ####################################\n",
    "# plt.figure()\n",
    "\n",
    "# # plot histogram of corr_z\n",
    "# plt.subplot(121)\n",
    "# plt.hist(corr_z, bins=50)\n",
    "\n",
    "# # smae for corr_array\n",
    "# plt.subplot(122)\n",
    "# plt.hist(corr_array, bins=50)\n",
    "# plt.plot([corr[0],corr[0]],[0,100],'r--',linewidth=3)\n",
    "\n",
    "# #plt.semilogy()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8901bbc-e5fe-468b-a7e1-4bf0b6dfa0de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500,)\n"
     ]
    }
   ],
   "source": [
    "# sort cz and plot\n",
    "cz = np.array(cz)\n",
    "idx = np.argsort(cz)[::-1]\n",
    "print (idx.shape)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(cz[idx])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302a0cd5-f775-4de3-af16-c43ee9fca702",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586d15b3-2cba-4110-a764-d2365b709779",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c8615c65-eec8-4da5-8d4d-b37482021ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#ttl times:  (90000,)\n",
      "#Video times:  (89155,)\n"
     ]
    }
   ],
   "source": [
    "def load_ttl_and_video_times_absolute(fname_results,\n",
    "                                      fname_video):\n",
    "\n",
    "    d = np.load(fname_results, allow_pickle=True)\n",
    "\n",
    "    ttl_times = d['ttl_times']\n",
    "\n",
    "    vid = np.load(fname_video, allow_pickle=True)\n",
    "\n",
    "    vid_times = vid['vide_frame_times']\n",
    "\n",
    "    #print (vid_times)\n",
    "\n",
    "    vid_times = vid_times[:,1]\n",
    "    vid_times = vid_times - vid_times[0]+ttl_times[0]\n",
    "\n",
    "    print (\"#ttl times: \", ttl_times.shape)\n",
    "    print (\"#Video times: \", times.shape)\n",
    "\n",
    "    return ttl_times, vid_times\n",
    "\n",
    "\n",
    "#\n",
    "fname_results = '/home/cat/data/bmi/DON-011733/20230309/results.npz'\n",
    "fname_video = '/home/cat/data/bmi/DON-011733/20230309/video_data.npz'\n",
    "\n",
    "ttl_times, vid_times = load_ttl_and_video_times_absolute(fname_results,\n",
    "                                                            fname_video)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4429b507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  32.7636478    32.7895813    32.82049561 ... 3033.96480322 3034.00370026\n",
      " 3034.03162456]\n",
      "(89155,)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35794ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
